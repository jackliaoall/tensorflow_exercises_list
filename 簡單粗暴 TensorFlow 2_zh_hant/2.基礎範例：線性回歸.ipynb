{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2.基礎範例：線性回歸.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOXrBb4Y6gHYc/Ugablunzi"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"HX_jGwbqGMFd","colab_type":"text"},"source":["考慮一個實際問題，某城市在 2013 年 - 2017 年的房價如下表所示：\n","\n","年份\n","\n","2013\n","\n","2014\n","\n","2015\n","\n","2016\n","\n","2017\n","\n","房價\n","\n","12000\n","\n","14000\n","\n","15000\n","\n","16500\n","\n","17500\n","\n","現在，我們希望通過對該資料進行線性回歸，即使用線性模型 y = ax + b 來擬合上述資料，此處 a 和 b 是待求的參數。\n","\n","首先，我們定義資料，進行基本的正規化操作。"]},{"cell_type":"code","metadata":{"id":"sZXX_2KRGMVk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"executionInfo":{"status":"ok","timestamp":1600179583728,"user_tz":-480,"elapsed":3382,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}},"outputId":"f3698e89-ab17-4a5a-cd0e-ccf52d2b99cf"},"source":["import numpy as np\n","import tensorflow as tf\n","\n","X_raw = np.array([2013, 2014, 2015, 2016, 2017], dtype=np.float32)\n","y_raw = np.array([12000, 14000, 15000, 16500, 17500], dtype=np.float32)\n","\n","X = (X_raw - X_raw.min()) / (X_raw.max() - X_raw.min())\n","y = (y_raw - y_raw.min()) / (y_raw.max() - y_raw.min())\n","\n","print(X)\n","print(y)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["[0.   0.25 0.5  0.75 1.  ]\n","[0.         0.36363637 0.54545456 0.8181818  1.        ]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uKIN4xWfGVEc","colab_type":"text"},"source":["回顧機器學習的基礎知識，對於多元函數 求局部極小值，梯度下降 的過程如下：\n","\n","初始化自變數為 x_0 ， k=0\n","\n","疊代進行下列步驟直到滿足收斂條件：\n","\n","求函数 f(x) 關於自變數的梯度 \\nabla f(x_k)\n","\n","更新自變數： x_{k+1} = x_{k} - \\gamma \\nabla f(x_k) 。這裡 \\gamma 是學習率（也就是梯度下降一次邁出的 “步長” 大小）\n","\n","k \\leftarrow k+1\n","\n","接下來，我們考慮如何使用程式來實現梯度下降方法，求得線性回歸的解 \\min_{a, b} L(a, b) = \\sum_{i=1}^n(ax_i + b - y_i)^2 "]},{"cell_type":"markdown","metadata":{"id":"J8NXCVE_Geu0","colab_type":"text"},"source":["**NumPy 下的線性回歸**"]},{"cell_type":"code","metadata":{"id":"CEo_TE4jGVUc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600180582057,"user_tz":-480,"elapsed":1041,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}},"outputId":"29682000-1fea-4af4-f52a-f6ee8df9a09e"},"source":["a, b = 0, 0\n","\n","num_epoch = 10000\n","learning_rate = 5e-4\n","for e in range(num_epoch):\n","    # 手動計算損失函數關於自變數（模型參數）的梯度\n","    y_pred = a * X + b\n","    grad_a, grad_b = 2 * (y_pred - y).dot(X), 2 * (y_pred - y).sum()\n","\n","    # 更新參數\n","    a, b = a - learning_rate * grad_a, b - learning_rate * grad_b\n","\n","print(a, b)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["0.9763702027872221 0.057564988311377796\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lwKspfs6Gh0s","colab_type":"text"},"source":["**TensorFlow 下的線性回歸**\n","\n","TensorFlow 的 即時執行模式 5 與上述 NumPy 的運行方式十分類似，然而提供了更快速的運算（GPU 支援）、自動推導、優化器等一系列對深度學習非常重要的功能。以下展示了如何使用 TensorFlow 計算線性回歸。可以注意到，程式的結構和前述 NumPy 的實現非常類似。這裡，TensorFlow 幫助我們做了兩件重要的工作：\n","\n","使用 tape.gradient(ys, xs) 自動計算梯度；\n","\n","使用 optimizer.apply_gradients(grads_and_vars) 自動更新模型參數"]},{"cell_type":"code","metadata":{"id":"Zs9dSEgHGiEc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600180597602,"user_tz":-480,"elapsed":15023,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}},"outputId":"0a6db562-fc33-43ad-aed8-3138831f7bf8"},"source":["X = tf.constant(X)\n","y = tf.constant(y)\n","\n","a = tf.Variable(initial_value=0.)\n","b = tf.Variable(initial_value=0.)\n","variables = [a, b]\n","\n","num_epoch = 10000\n","optimizer = tf.keras.optimizers.SGD(learning_rate=5e-4)\n","for e in range(num_epoch):\n","    # 使用tf.GradientTape()記錄損失函數的梯度資訊\n","    with tf.GradientTape() as tape:\n","        y_pred = a * X + b\n","        loss = tf.reduce_sum(tf.square(y_pred - y))\n","    # TensorFlow自動計算損失函數關於自變數（模型參數）的梯度\n","    grads = tape.gradient(loss, variables)\n","    # TensorFlow自動根據梯度更新參數\n","    optimizer.apply_gradients(grads_and_vars=zip(grads, variables))\n","\n","print(a, b)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.97637> <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.057565063>\n"],"name":"stdout"}]}]}