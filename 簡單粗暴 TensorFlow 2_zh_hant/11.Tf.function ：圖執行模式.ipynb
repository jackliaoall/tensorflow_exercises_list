{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"11.Tf.function ：圖執行模式.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNPNcTKmlxEDUO/IMw2m6LM"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"JaJORkysbWoZ"},"source":["雖然預設的即時執行模式（Eager Execution）為我們帶來了靈活及容易偵錯的特性，但在特定的場合，例如追求高性能或部署模型時，我們依然希望使用 TensorFlow 1.X 中預設的圖執行模式（Graph Execution），將模型轉換為高效的 TensorFlow 圖模型。此時，TensorFlow 2 為我們提供了 tf.function 模組，結合 AutoGraph 機制，使得我們僅需加入一個簡單的 @tf.function 修飾符，就能輕鬆將模型以圖執行模式運行。"]},{"cell_type":"markdown","metadata":{"id":"0lHLPiOUbqHW"},"source":["並不是任何函數都可以被 @tf.function 修飾！@tf.function 使用靜態編譯將函數內的程式碼轉換成計算圖，因此對函數內可使用的語句有一定限制（僅支援 Python 語言的一個子集），且需要函數內的操作本身能夠被建構為計算圖。建議在函數內只使用 TensorFlow 的原生操作，不要使用過於複雜的 Python 語句，函數參數只包括 TensorFlow 張量或 NumPy 陣列，並最好是能夠按照計算圖的思想去建構函數（換言之，@tf.function 只是給了你一種更方便的寫計算圖的方法，而不是一顆能給任何函數加速的 銀子彈 ）。詳細內容可參考 AutoGraph Capabilities and Limitations 。建議配合 附錄 一同閱讀本節以獲得較深入的理解。"]},{"cell_type":"code","metadata":{"id":"LulpJMB8bwT0"},"source":["import tensorflow as tf\n","import time\n","from zh.model.mnist.cnn import CNN\n","from zh.model.utils import MNISTLoader\n","\n","num_batches = 1000\n","batch_size = 50\n","learning_rate = 0.001\n","data_loader = MNISTLoader()\n","model = CNN()\n","optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","\n","@tf.function\n","def train_one_step(X, y):    \n","    with tf.GradientTape() as tape:\n","        y_pred = model(X)\n","        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n","        loss = tf.reduce_mean(loss)\n","        # 注意這裡使用了TensorFlow內建的tf.print()。@tf.function不支援Python內建的print方法\n","        tf.print(\"loss\", loss)\n","    grads = tape.gradient(loss, model.variables)    \n","    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))\n","\n","start_time = time.time()\n","for batch_index in range(num_batches):\n","    X, y = data_loader.get_batch(batch_size)\n","    train_one_step(X, y)\n","end_time = time.time()\n","print(end_time - start_time)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HGIhqD6IcBBm"},"source":["**tf.function 內在機制**\n","\n","當被 @tf.function 修飾的函數第一次被呼叫的時候，進行以下操作：\n","\n","在即時執行模式關閉的環境下，函數內的程式碼依次運行。也就是說，每個 tf. 方法都只是定義了計算節點，而並沒有進行任何實質的計算。這與 TensorFlow 1.X 的圖執行模式是一致的；\n","\n","使用 AutoGraph 將函數中的 Python 控制語句轉換成 TensorFlow 計算圖中的對應節點（比如說 while 和 for 語句轉換為 tf.while ， if 語句轉換為 tf.cond 等等；\n","\n","基於上面的兩步，建立函數內程式碼的計算圖表示（為了保證圖的計算順序，圖中還會自動加入一些 tf.control_dependencies 節點）；\n","\n","運行一次這個計算圖；\n","\n","基於函數的名字和輸入的函數參數的類型生成一個雜湊值，並將建立的計算圖緩衝區到一個雜湊表中。\n","\n","以下是一個測試題："]},{"cell_type":"code","metadata":{"id":"Bu2JIltAcPdt","executionInfo":{"status":"ok","timestamp":1601477177106,"user_tz":-480,"elapsed":4149,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}},"outputId":"ee0b38f7-a482-4fd2-cf3a-808cd1bcce65","colab":{"base_uri":"https://localhost:8080/","height":134}},"source":["import tensorflow as tf\n","import numpy as np\n","\n","@tf.function\n","def f(x):\n","    print(\"The function is running in Python\")\n","    tf.print(x)\n","\n","a = tf.constant(1, dtype=tf.int32)\n","f(a)\n","b = tf.constant(2, dtype=tf.int32)\n","f(b)\n","b_ = np.array(2, dtype=np.int32)\n","f(b_)\n","c = tf.constant(0.1, dtype=tf.float32)\n","f(c)\n","d = tf.constant(0.2, dtype=tf.float32)\n","f(d)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The function is running in Python\n","1\n","2\n","2\n","The function is running in Python\n","0.1\n","0.2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Pr6Z3cdInVY4"},"source":["**AutoGraph：將 Python 控制流轉換為 TensorFlow 計算圖**\n","\n","前面提到，@tf.function 使用名為 AutoGraph 的機制將函數中的 Python 控制流語句轉換成 TensorFlow 計算圖中的對應節點。以下是一個範例，使用 tf.autograph 模組的低層 API tf.autograph.to_code 將函數 square_if_positive 轉換成 TensorFlow 計算圖"]},{"cell_type":"code","metadata":{"id":"T-VFkDMXnVjE","executionInfo":{"status":"ok","timestamp":1601529838783,"user_tz":-480,"elapsed":3417,"user":{"displayName":"Liao Jack","photoUrl":"","userId":"16157886839679822522"}},"outputId":"d51d2115-10d5-4a09-88e0-e7acf5aee63c","colab":{"base_uri":"https://localhost:8080/","height":524}},"source":["import tensorflow as tf\n","\n","@tf.function\n","def square_if_positive(x):\n","    if x > 0:\n","        x = x * x\n","    else:\n","        x = 0\n","    return x\n","\n","a = tf.constant(1)\n","b = tf.constant(-1)\n","print(square_if_positive(a), square_if_positive(b))\n","print(tf.autograph.to_code(square_if_positive.python_function))"],"execution_count":1,"outputs":[{"output_type":"stream","text":["tf.Tensor(1, shape=(), dtype=int32) tf.Tensor(0, shape=(), dtype=int32)\n","def tf__square_if_positive(x):\n","    with ag__.FunctionScope('square_if_positive', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:\n","        do_return = False\n","        retval_ = ag__.UndefinedReturnValue()\n","\n","        def get_state():\n","            return (x,)\n","\n","        def set_state(vars_):\n","            nonlocal x\n","            (x,) = vars_\n","\n","        def if_body():\n","            nonlocal x\n","            x = (ag__.ld(x) * ag__.ld(x))\n","\n","        def else_body():\n","            nonlocal x\n","            x = 0\n","        ag__.if_stmt((ag__.ld(x) > 0), if_body, else_body, get_state, set_state, ('x',), 1)\n","        try:\n","            do_return = True\n","            retval_ = ag__.ld(x)\n","        except:\n","            do_return = False\n","            raise\n","        return fscope.ret(retval_, do_return)\n","\n"],"name":"stdout"}]}]}